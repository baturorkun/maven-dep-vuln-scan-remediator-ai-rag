
# Project ENVs ( Optional )
# PROJECT_TITLE=
# PROJECT_LOGO=

# LLM Configuration for agent.py
# Choose one of the following setups:

# Option 1: Ollama (local, no API key needed)
export LLM_BASE_URL=http://localhost:11434/v1
export LLM_MODEL=qwen3:8b
# LLM_API_KEY not needed

# Option 2: OpenAI
# export LLM_BASE_URL=https://api.openai.com/v1
# export LLM_MODEL=gpt-4
# export LLM_API_KEY=sk-your-api-key-here

# Option 3: Anthropic Claude (via OpenAI-compatible proxy)
# export LLM_BASE_URL=https://api.anthropic.com/v1
# export LLM_MODEL=claude-3-5-sonnet-20241022
# export LLM_API_KEY=sk-ant-your-api-key-here

# Option 4: vLLM (local server)
# export LLM_BASE_URL=http://localhost:8000/v1
# export LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
# export LLM_API_KEY not needed

# Option 5: Other OpenAI-compatible endpoints
# export LLM_BASE_URL=your-endpoint-url
# export LLM_MODEL=your-model-name
# export LLM_API_KEY=your-api-key (if required)

# Neo4j Configuration
# When running in container, use host.containers.internal to access host services
# When running locally without container, use localhost
export NEO4J_URI=bolt://host.containers.internal:7687
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=password


export CVE_LOOKUP_ONLINE=false
